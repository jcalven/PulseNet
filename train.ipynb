{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=(PendingDeprecationWarning, DeprecationWarning))\n",
    "\n",
    "# Import wavenetclass modules\n",
    "from configdict import ConfigDict\n",
    "from model import PulseNet\n",
    "from dataset import PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_files, val_files, hparams, pipeline_settings, logs_path='./logs/', experiment_name='PulseNet_default'):\n",
    "    \"\"\"Main method for training PulseNet model.\"\"\"\n",
    "    \n",
    "    preprocess = PreProcess(**pipeline_settings)\n",
    "    \n",
    "    # Prep train and eval data\n",
    "    train_dataset = preprocess.prep(train_files)\n",
    "    val_dataset = preprocess.prep(val_files)\n",
    "\n",
    "    # Dataset iterator\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "    train_data_X, train_data_y = train_dataset.make_one_shot_iterator().get_next()\n",
    "    val_data_X, val_data_y = val_dataset.make_one_shot_iterator().get_next()\n",
    "    data_X, data_y = iterator.get_next()\n",
    "\n",
    "    # Initialize with required Datasets\n",
    "    train_iterator = iterator.make_initializer(train_dataset)\n",
    "    val_iterator = iterator.make_initializer(val_dataset)\n",
    "\n",
    "    # Length of wavelength\n",
    "    X_length = preprocess.stop_index - preprocess.start_index \n",
    "    \n",
    "    placeholder_X = tf.placeholder(tf.float32, [None, X_length, 1])\n",
    "    placeholder_y = tf.placeholder(tf.int64, [None, 2])\n",
    "    \n",
    "    # Custom summaries\n",
    "    train_epoch_acc_summary = tf.Summary()\n",
    "    train_epoch_auc_summary = tf.Summary()\n",
    "    val_epoch_acc_summary = tf.Summary()\n",
    "    val_epoch_auc_summary = tf.Summary()\n",
    "\n",
    "    loss_arr = []\n",
    "    acc_arr = []\n",
    "    epoch_arr = []\n",
    "\n",
    "    time_string = datetime.datetime.now().isoformat()\n",
    "    experiment_name += '_{}'.format(time_string)\n",
    "\n",
    "    # Instantiate model\n",
    "    model = PulseNet(data_X, data_y, hparams=hparams, run_dir=logs_path, learning_rate=hparams.learning_rate, experiment_name=experiment_name)\n",
    "    \n",
    "    print('Experiment name:', experiment_name, '\\n')\n",
    "    print('Starting training...\\n')\n",
    "    \n",
    "    # Run session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        # Write logs to Tensorboard\n",
    "        train_writer = tf.summary.FileWriter(logs_path+'train/'+experiment_name, sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(logs_path+'val/'+experiment_name, sess.graph)\n",
    "        \n",
    "        n_train_samples = 0\n",
    "        n_val_samples = 0\n",
    "\n",
    "        for epoch_no in range(hparams.EPOCHS):\n",
    "\n",
    "            train_loss, train_accuracy = 0, 0\n",
    "            val_loss, val_accuracy = 0, 0\n",
    "\n",
    "            X_train, y_train = sess.run((train_data_X, train_data_y))\n",
    "            X_val, y_val = sess.run((val_data_X, val_data_y))\n",
    "\n",
    "            # Initialize iterator with training data\n",
    "            sess.run(train_iterator, feed_dict = {placeholder_X: X_train, placeholder_y: y_train})\n",
    "\n",
    "            i_batch = 0\n",
    "            try:\n",
    "                with tqdm(total = len(y_train)) as pbar:\n",
    "                    while i_batch <= hparams.n_train_batches:\n",
    "                        _, loss, acc_update_op, summary = sess.run([model.optimizer, model.loss, model.accuracy_update_op,\n",
    "                                                 model.summaries])\n",
    "                        train_loss += loss \n",
    "                        n_train_samples += pipeline_settings['batch']\n",
    "                        pbar.update(pipeline_settings['batch'])\n",
    "                        \n",
    "                        if i_batch % pipeline_settings.train_sample_rate == 0:\n",
    "                            print('\\nEpoch {}: batch = {}, loss = {:.4f}, accuracy = {:.4f}'.format(epoch_no, i_batch+1, train_loss/(i_batch+1), acc_update_op))\n",
    "                            # Write logs at every iteration\n",
    "                            train_writer.add_summary(summary, n_train_samples)\n",
    "\n",
    "                        i_batch += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('End of dataset')\n",
    "\n",
    "            # After every epoch, calculate the accuracy of the last seen training batch \n",
    "            acc_train = sess.run(model.accuracy)\n",
    "\n",
    "            # Add logs at end of train epoch\n",
    "            train_epoch_acc_summary.value.add(tag=\"epoch_accuracy\", simple_value=acc_train)\n",
    "            train_writer.add_summary(train_epoch_acc_summary, epoch_no)\n",
    "\n",
    "            print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(epoch_no, loss, acc_train * 100))\n",
    "\n",
    "            # Save model\n",
    "            print('\\nSaving model...')\n",
    "            model.saver.save(sess, 'logs/checkpoints/'+experiment_name+'/model')\n",
    "            print('ok\\n')\n",
    "\n",
    "            # Initialize iterator with validation data\n",
    "            sess.run(val_iterator, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
    "\n",
    "            i_batch = 1\n",
    "            try:\n",
    "                with tqdm(total = len(y_val)) as pbar:\n",
    "                    while i_batch <= hparams.n_train_batches:\n",
    "                        loss, val_acc_update_op, summary, auc, auc_update_op = sess.run([model.loss, model.accuracy_update_op,\n",
    "                                                 model.summaries, model.auc, model.auc_update_op])\n",
    "                        val_loss += loss\n",
    "                        n_val_samples += pipeline_settings['batch']\n",
    "                        pbar.update(pipeline_settings['batch'])\n",
    "\n",
    "                        # Write logs at every iteration\n",
    "                        val_writer.add_summary(summary, n_val_samples)\n",
    "\n",
    "                        i_batch += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('End of dataset')\n",
    "\n",
    "            # After each epoch, calculate the accuracy of the test data\n",
    "            acc_val, auc_val = sess.run([model.accuracy, model.auc])\n",
    "            print('AUC =', auc_val)\n",
    "\n",
    "            auc_local_variables = [str(i.name) for i in tf.local_variables() if str(i.name).startswith('auc')]\n",
    "            roc_dict = {vn.split('/')[-1].split(':')[0]: sess.run(tf.get_default_graph().get_tensor_by_name(vn)) for vn in auc_local_variables}\n",
    "\n",
    "            # Add logs at end of train epoch\n",
    "            val_epoch_acc_summary.value.add(tag=\"epoch_accuracy\", simple_value=acc_val)\n",
    "            val_epoch_auc_summary.value.add(tag=\"epoch_AUC\", simple_value=auc_val)\n",
    "            val_writer.add_summary(val_epoch_acc_summary, epoch_no)\n",
    "            val_writer.add_summary(val_epoch_auc_summary, epoch_no)\n",
    "\n",
    "            print(\"Average validation set accuracy and loss over {} batch iterations are {:.2f}% and {:.2f}\".format(i_batch, acc_val * 100, val_loss / i_batch))\n",
    "        \n",
    "        # Calculate and save True Positive Rate (TPR) and False Positive Rate (FPR)\n",
    "        tpr = roc_dict['true_positives'] / (roc_dict['true_positives'] + roc_dict['false_negatives'])\n",
    "        tnr = roc_dict['true_negatives'] / (roc_dict['true_negatives'] + roc_dict['false_positives'])\n",
    "        fpr = 1 - tnr\n",
    "        \n",
    "        roc_dict['auc'] = auc_val\n",
    "        roc_dict['tpr'] = tpr\n",
    "        roc_dict['tnr'] = fpr\n",
    "        roc_dict['fpr'] = fpr\n",
    "        \n",
    "        log_data_path = logs_path + 'checkpoints' + '/' + experiment_name + '/'\n",
    "        \n",
    "         # Dump AUC ROC data to json\n",
    "        with open(log_data_path + 'roc_auc.json', 'w') as fp:\n",
    "            roc_dict_as_list = {key: val.tolist() for key, val in roc_dict.items()}\n",
    "            json.dump(roc_dict_as_list, fp)\n",
    "        \n",
    "        # Dump configuration to json\n",
    "        with open(log_data_path + 'config.json', 'w') as fp:\n",
    "            json.dump({'hparams': hparams, 'pipeline_settings': pipeline_settings}, fp)\n",
    "\n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "        \n",
    "        print('\\nTraining finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main(experiment_name, pipeline_settings=None, hparams=None):\n",
    "    \n",
    "    # Default input pipeline settings ###\n",
    "    pipeline_settings_run = ConfigDict(\n",
    "        buffer_size = 1, #100,\n",
    "        num_parallel_calls = 1, #20,\n",
    "        prefetch = 1, #10,\n",
    "        cycle_length = 1, #2,\n",
    "        repeat = 1,\n",
    "        batch=1, #128,\n",
    "        stop_index=10,\n",
    "        train_sample_rate=10)\n",
    "    \n",
    "    # Update pipeline_settings_run parameters\n",
    "    if isinstance(hparams, dict):\n",
    "        for key, val in pipeline_settings.items():\n",
    "            if key in pipeline_settings_run:\n",
    "                pipeline_settings_run.update({key:val})\n",
    "\n",
    "    # Default hyper parameters\n",
    "    hparams_run = ConfigDict(\n",
    "        dilation_kernel_width = 4, #2\n",
    "        skip_output_dim = 2, #1\n",
    "        preprocess_output_size = 1,\n",
    "        preprocess_kernel_width = 4, #1\n",
    "        num_residual_blocks = 3,\n",
    "        dilation_rates = [1, 2, 4],\n",
    "        EPOCHS = 3,\n",
    "        n_train_batches=500,\n",
    "        n_val_batches=500,\n",
    "        learning_rate=0.002)#0.001)\n",
    "    \n",
    "    # Update hparams_run parameters\n",
    "    if isinstance(hparams, dict):\n",
    "        for key, val in hparams.items():\n",
    "            if key in hparams_run:\n",
    "                hparams_run.update({key:val})\n",
    "            \n",
    "    print('###################################')\n",
    "    print('          PulseNet v0.1            ')\n",
    "    print('\\nPipeline settings:')\n",
    "    for key, val in pipeline_settings_run.items():\n",
    "        print(key, '=', val)\n",
    "    print('\\nHyper parameters:')\n",
    "    for key, val in hparams_run.items():\n",
    "        print(key, '=', val)\n",
    "    print()        \n",
    "\n",
    "    # Paths\n",
    "    SRC_PATH = '/cfs/klemming/nobackup/j/jcalven/lar/notebooks/wavenetclass/'\n",
    "    DATA_PATH = SRC_PATH+'data/'\n",
    "    LOGS_PATH = SRC_PATH+'version_0_1/logs/'\n",
    "    \n",
    "    \n",
    "    # Data files\n",
    "    train_files = ['path/to/train_data.h5']\n",
    "    val_files = ['path/to/val_data.h5']\n",
    "\n",
    "    \n",
    "    # Run training\n",
    "    roc_dict = run(train_files=train_files, val_files=val_files, hparams=hparams_run, \n",
    "        pipeline_settings=pipeline_settings_run, logs_path=LOGS_PATH, experiment_name=experiment_name)\n",
    "    \n",
    "    return roc_dict\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    experiment_name = 'test_clean'\n",
    "    \n",
    "    roc_dict = run_main(experiment_name, pipeline_settings=None, hparams=None)\n",
    "    \n",
    "    plot = False\n",
    "    \n",
    "    if plot:\n",
    "        tpr = roc_dict['true_positives'] / (roc_dict['true_positives'] + roc_dict['false_negatives'])\n",
    "        tnr = roc_dict['true_negatives'] / (roc_dict['true_negatives'] + roc_dict['false_positives'])\n",
    "        fpr = 1 - tnr\n",
    "\n",
    "        with plt.style.context(('bmh')):\n",
    "            fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6,6))\n",
    "            ax.plot(fpr, tpr, label='1 epoch')\n",
    "            ax.plot([0,1], [0,1], '--')\n",
    "            ax.text(x=0.81, y=0.21, s='AUC = {:0.2}'.format(roc_dict['auc']))\n",
    "            ax.set_xlim(0.,1.)\n",
    "            ax.set_ylim(0.,1.)\n",
    "            ax.set_title('ROC')\n",
    "            ax.set_xlabel('False Positive Rate (1 - specificity)')\n",
    "            ax.set_ylabel('True Positive Rate (sensitivity)')\n",
    "            fig.savefig(experiment_name +'.pdf', bbox_inches='tight')\n",
    "            fig.savefig(experiment_name +'.png', dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
